{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/siyinggu/LLM_Finetuning/blob/main/QLoRA_Evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83IODv6oqTDK",
        "outputId": "f2ab2f5e-b81e-4a08-e8fa-c4583b825a32"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: adapters in /usr/local/lib/python3.10/dist-packages (0.2.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.20.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.32.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.23.5)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.5.82)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.17.5)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.2.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.10.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.7.4)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
            "Looking in indexes: https://pypi.org/simple/\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.43.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.3.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.25.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->bitsandbytes) (12.5.82)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->bitsandbytes) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install adapters datasets transformers\n",
        "!pip install accelerate\n",
        "!pip install wandb\n",
        "!pip install -i https://pypi.org/simple/ bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "0a01d4e3c33d452daa9723a04fc90c55",
            "e081ad14de854dc884428ab1eeb05cee",
            "9e2019b0a84e41cb9f4f1d365d90c46e",
            "7dc2207930834e89a6338c6b632dee2f",
            "4d1a17cee8714460950144f8ec53f024",
            "8b5f0f8019c34331b33d1b680231ae55",
            "5ee7153aeb724fe49b58c7e761b17567",
            "b4f5bbc7cdc44e62a136a57e51494300",
            "6a667649db264592a0e6313cd20516bc",
            "0d577d5664a84c0e8bdfae26f2896c64",
            "553ad191aa7d4f75ac02165e1919d169",
            "f514f2b3e7c941699ac363ffcec364e7",
            "8c22bc4613e549a6aa0cf49c271b9d6d",
            "0cc3ea6ce2f54fa1a97f3c9f91425854",
            "eb7d4e2c1e1a432589e6855108a18d8e",
            "1a4d02d8ba7344b49b28093f3dc34be4"
          ]
        },
        "id": "E4IJro4ermUB",
        "outputId": "2a29090d-6b84-42e4-97c6-88f7c355d7e7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:n7p1wxc3) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0a01d4e3c33d452daa9723a04fc90c55",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>memory_usage</td><td>▁</td></tr><tr><td>time</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>memory_usage</td><td>2720.79688</td></tr><tr><td>time</td><td>1721475444.21123</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">happy-disco-3</strong> at: <a href='https://wandb.ai/siyinggu-nyu/QLoRA_New/runs/n7p1wxc3' target=\"_blank\">https://wandb.ai/siyinggu-nyu/QLoRA_New/runs/n7p1wxc3</a><br/> View project at: <a href='https://wandb.ai/siyinggu-nyu/QLoRA_New' target=\"_blank\">https://wandb.ai/siyinggu-nyu/QLoRA_New</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240720_113706-n7p1wxc3/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:n7p1wxc3). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.5"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240720_113748-gu6o7ux3</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/siyinggu-nyu/QLoRA_New/runs/gu6o7ux3' target=\"_blank\">dainty-frog-4</a></strong> to <a href='https://wandb.ai/siyinggu-nyu/QLoRA_New' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/siyinggu-nyu/QLoRA_New' target=\"_blank\">https://wandb.ai/siyinggu-nyu/QLoRA_New</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/siyinggu-nyu/QLoRA_New/runs/gu6o7ux3' target=\"_blank\">https://wandb.ai/siyinggu-nyu/QLoRA_New/runs/gu6o7ux3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "Name                     Architecture         #Param      %Param  Active   Train\n",
            "--------------------------------------------------------------------------------\n",
            "assistant_adapter        lora              4,718,592      10.456       1       1\n",
            "--------------------------------------------------------------------------------\n",
            "Full model                                45,129,216     100.000               0\n",
            "================================================================================\n",
            "DistilBertForSequenceClassification(\n",
            "  (distilbert): DistilBertModel(\n",
            "    (embeddings): Embeddings(\n",
            "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "      (position_embeddings): Embedding(512, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (transformer): Transformer(\n",
            "      (layer): ModuleList(\n",
            "        (0-5): 6 x TransformerBlockWithAdapters(\n",
            "          (attention): MultiHeadSelfAttentionWithAdapters(\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (q_lin): LoRALinear4bit(\n",
            "              in_features=768, out_features=768, bias=True\n",
            "              (loras): ModuleDict(\n",
            "                (assistant_adapter): LoRA(\n",
            "                  (lora_dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (k_lin): LoRALinear4bit(\n",
            "              in_features=768, out_features=768, bias=True\n",
            "              (loras): ModuleDict(\n",
            "                (assistant_adapter): LoRA(\n",
            "                  (lora_dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (v_lin): LoRALinear4bit(\n",
            "              in_features=768, out_features=768, bias=True\n",
            "              (loras): ModuleDict(\n",
            "                (assistant_adapter): LoRA(\n",
            "                  (lora_dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (out_lin): Linear4bit(in_features=768, out_features=768, bias=True)\n",
            "            (prefix_tuning): PrefixTuningLayer(\n",
            "              (prefix_gates): ModuleDict()\n",
            "              (pool): PrefixTuningPool(\n",
            "                (prefix_tunings): ModuleDict()\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (ffn): FFN(\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (lin1): LoRALinear4bit(\n",
            "              in_features=768, out_features=3072, bias=True\n",
            "              (loras): ModuleDict(\n",
            "                (assistant_adapter): LoRA(\n",
            "                  (lora_dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (lin2): LoRALinear4bit(\n",
            "              in_features=3072, out_features=768, bias=True\n",
            "              (loras): ModuleDict(\n",
            "                (assistant_adapter): LoRA(\n",
            "                  (lora_dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (activation): GELUActivation()\n",
            "          )\n",
            "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (attention_adapters): BottleneckLayer(\n",
            "            (adapters): ModuleDict()\n",
            "            (adapter_fusion_layer): ModuleDict()\n",
            "          )\n",
            "          (output_adapters): BottleneckLayer(\n",
            "            (adapters): ModuleDict()\n",
            "            (adapter_fusion_layer): ModuleDict()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (shared_parameters): ModuleDict()\n",
            "    (invertible_adapters): ModuleDict()\n",
            "    (prefix_tuning): PrefixTuningPool(\n",
            "      (prefix_tunings): ModuleDict()\n",
            "    )\n",
            "    (prompt_tuning): PromptTuningLayer(\n",
            "      (base_model_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "      (prompt_tunings): ModuleDict()\n",
            "    )\n",
            "  )\n",
            "  (pre_classifier): Linear4bit(in_features=768, out_features=768, bias=True)\n",
            "  (classifier): CastOutputToFloat(\n",
            "    (0): Linear(in_features=768, out_features=3, bias=True)\n",
            "  )\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            ")\n",
            "torch.bfloat16 23836416 0.47534226947643365\n",
            "torch.float32 4780803 0.0953380637399407\n",
            "torch.uint8 21528576 0.42931966678362565\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['id', 'text', 'label', 'sentiment'],\n",
            "        num_rows: 31232\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['id', 'text', 'label', 'sentiment'],\n",
            "        num_rows: 5205\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['id', 'text', 'label', 'sentiment'],\n",
            "        num_rows: 5206\n",
            "    })\n",
            "})\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1788' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1788/1875 1:11:00 < 03:27, 0.42 it/s, Epoch 0.92/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>187</td>\n",
              "      <td>0.719400</td>\n",
              "      <td>0.705315</td>\n",
              "      <td>0.862500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>374</td>\n",
              "      <td>0.686800</td>\n",
              "      <td>0.678827</td>\n",
              "      <td>0.872419</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>561</td>\n",
              "      <td>0.735200</td>\n",
              "      <td>0.665283</td>\n",
              "      <td>0.876258</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>748</td>\n",
              "      <td>0.719000</td>\n",
              "      <td>0.645890</td>\n",
              "      <td>0.884490</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>935</td>\n",
              "      <td>0.619800</td>\n",
              "      <td>0.653885</td>\n",
              "      <td>0.887733</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1122</td>\n",
              "      <td>0.641000</td>\n",
              "      <td>0.642718</td>\n",
              "      <td>0.885970</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1309</td>\n",
              "      <td>0.687200</td>\n",
              "      <td>0.626735</td>\n",
              "      <td>0.890733</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1496</td>\n",
              "      <td>0.691300</td>\n",
              "      <td>0.616361</td>\n",
              "      <td>0.893146</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1683</td>\n",
              "      <td>0.682300</td>\n",
              "      <td>0.618754</td>\n",
              "      <td>0.894416</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1875' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1875/1875 1:16:24, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>187</td>\n",
              "      <td>0.719400</td>\n",
              "      <td>0.705315</td>\n",
              "      <td>0.862500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>374</td>\n",
              "      <td>0.686800</td>\n",
              "      <td>0.678827</td>\n",
              "      <td>0.872419</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>561</td>\n",
              "      <td>0.735200</td>\n",
              "      <td>0.665283</td>\n",
              "      <td>0.876258</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>748</td>\n",
              "      <td>0.719000</td>\n",
              "      <td>0.645890</td>\n",
              "      <td>0.884490</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>935</td>\n",
              "      <td>0.619800</td>\n",
              "      <td>0.653885</td>\n",
              "      <td>0.887733</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1122</td>\n",
              "      <td>0.641000</td>\n",
              "      <td>0.642718</td>\n",
              "      <td>0.885970</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1309</td>\n",
              "      <td>0.687200</td>\n",
              "      <td>0.626735</td>\n",
              "      <td>0.890733</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1496</td>\n",
              "      <td>0.691300</td>\n",
              "      <td>0.616361</td>\n",
              "      <td>0.893146</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1683</td>\n",
              "      <td>0.682300</td>\n",
              "      <td>0.618754</td>\n",
              "      <td>0.894416</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1870</td>\n",
              "      <td>0.634500</td>\n",
              "      <td>0.618558</td>\n",
              "      <td>0.893388</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6a667649db264592a0e6313cd20516bc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▃▄▆▇▆▇███</td></tr><tr><td>eval/loss</td><td>█▆▅▃▄▃▂▁▁▁</td></tr><tr><td>eval/runtime</td><td>▁█▃▄▄▄█▃▇▄</td></tr><tr><td>eval/samples_per_second</td><td>█▁▆▅▅▅▁▆▂▅</td></tr><tr><td>eval/steps_per_second</td><td>█▁▆▅▅▅▁▆▂▅</td></tr><tr><td>memory_usage</td><td>▁</td></tr><tr><td>time</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>▁▃▆▄▁▅▇▅▃▃▂▄▅▆█▆▃▄▄▆▃▃▃▆▄▄▇▆▆▄▁▄▃▄▅▅▃▄▅▅</td></tr><tr><td>train/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>█▇▄▃▃▃▃▃▃▂▂▂▃▃▃▂▂▂▂▂▃▂▃▃▂▂▂▁▄▂▂▂▂▃▂▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.89339</td></tr><tr><td>eval/loss</td><td>0.61856</td></tr><tr><td>eval/runtime</td><td>206.8541</td></tr><tr><td>eval/samples_per_second</td><td>25.163</td></tr><tr><td>eval/steps_per_second</td><td>25.163</td></tr><tr><td>memory_usage</td><td>3408.83984</td></tr><tr><td>time</td><td>1721475488.81647</td></tr><tr><td>total_flos</td><td>4408958269440000.0</td></tr><tr><td>train/epoch</td><td>0.96055</td></tr><tr><td>train/global_step</td><td>1875</td></tr><tr><td>train/grad_norm</td><td>1.10566</td></tr><tr><td>train/learning_rate</td><td>0.0002</td></tr><tr><td>train/loss</td><td>0.6345</td></tr><tr><td>train_loss</td><td>0.67383</td></tr><tr><td>train_runtime</td><td>4585.8575</td></tr><tr><td>train_samples_per_second</td><td>6.542</td></tr><tr><td>train_steps_per_second</td><td>0.409</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">dainty-frog-4</strong> at: <a href='https://wandb.ai/siyinggu-nyu/QLoRA_New/runs/gu6o7ux3' target=\"_blank\">https://wandb.ai/siyinggu-nyu/QLoRA_New/runs/gu6o7ux3</a><br/> View project at: <a href='https://wandb.ai/siyinggu-nyu/QLoRA_New' target=\"_blank\">https://wandb.ai/siyinggu-nyu/QLoRA_New</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20240720_113748-gu6o7ux3/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfrom transformers import logging\\nlogging.set_verbosity(logging.CRITICAL)\\n\\ndef prompt_model(model, text: str):\\n    batch = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\\n    batch = batch.to(model.device)\\n\\n    model.eval()\\n    with torch.inference_mode(), torch.cuda.amp.autocast():\\n        outputs = model(**batch)\\n\\n    return outputs\\n\\nprint(prompt_model(model, \"Explain Calculus to a primary school student\"))\\n\\nmodel.merge_adapter(\"assistant_adapter\")\\nprint(prompt_model(model, \"Explain NLP in simple terms\"))\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, BitsAndBytesConfig\n",
        "import pandas as pd\n",
        "import wandb\n",
        "###Step1: Model Initialization\n",
        "#1.1 Initialize a new wandb run\n",
        "wandb.init(project=\"QLoRA_New\", entity=\"siyinggu-nyu\")\n",
        "\n",
        "#1.2 modelpath for distilbert\n",
        "modelpath = \"distilbert-base-uncased\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    )\n",
        "#1.3 Load 4-bit quantized model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    modelpath,\n",
        "    quantization_config=bnb_config,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    num_labels=3,  # Update this if you have a different number of labels\n",
        "    low_cpu_mem_usage=True\n",
        ")\n",
        "model.config.use_cache = False\n",
        "\n",
        "#1.4 Initialize lora config\n",
        "import adapters\n",
        "from adapters import LoRAConfig\n",
        "\n",
        "adapters.init(model)\n",
        "\n",
        "config = LoRAConfig(\n",
        "    selfattn_lora=True, intermediate_lora=True, output_lora=True,\n",
        "    attn_matrices=[\"q\", \"k\", \"v\"],\n",
        "    alpha=16, r=64, dropout=0.1\n",
        ")\n",
        "model.add_adapter(\"assistant_adapter\", config=config)\n",
        "model.train_adapter(\"assistant_adapter\")\n",
        "\n",
        "print(model.adapter_summary())\n",
        "\n",
        "#1.5 Give model paramater size and type\n",
        "for param in model.parameters():\n",
        "    if param.ndim == 1:\n",
        "        # cast the small parameters (e.g. layernorm) to fp32 for stability\n",
        "        param.data = param.data.to(torch.float32)\n",
        "\n",
        "\n",
        "class CastOutputToFloat(torch.nn.Sequential):\n",
        "    def forward(self, x): return super().forward(x).to(torch.float32)\n",
        "model.classifier = CastOutputToFloat(model.classifier)\n",
        "\n",
        "print(model)\n",
        "\n",
        "# Verifying the datatypes.\n",
        "dtypes = {}\n",
        "for _, p in model.named_parameters():\n",
        "    dtype = p.dtype\n",
        "    if dtype not in dtypes:\n",
        "        dtypes[dtype] = 0\n",
        "    dtypes[dtype] += p.numel()\n",
        "total = 0\n",
        "for k, v in dtypes.items():\n",
        "    total += v\n",
        "for k, v in dtypes.items():\n",
        "    print(k, v, v / total)\n",
        "\n",
        "import os\n",
        "\n",
        "###Step2: Data preprocessing and tokenize\n",
        "from datasets import load_dataset, load_metric\n",
        "dataset = load_dataset(\"Sp1786/multiclass-sentiment-analysis-dataset\")\n",
        "\n",
        "##2.1: Clean dataset\n",
        "#: Function to clean text\n",
        "import re\n",
        "def clean_text(text):\n",
        "    if text is None:\n",
        "        return \"\"\n",
        "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
        "    text = re.sub(r'\\W', ' ', text)  # Remove special characters\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    return text.strip()\n",
        "\n",
        "# Function to clean the entire dataset\n",
        "def clean_dataset(dataset):\n",
        "    dataset = dataset.map(lambda x: {'text': clean_text(x['text'])})\n",
        "    return dataset\n",
        "\n",
        "cleaned_dataset = clean_dataset(dataset)\n",
        "print(cleaned_dataset)\n",
        "\n",
        "\n",
        "##2.2: Tokenize dataset\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(modelpath)\n",
        "# Ensure the tokenizer has a pad token\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "# Tokenize function\n",
        "def tokenize_func(examples):\n",
        "    return tokenizer(\n",
        "        examples['text'],\n",
        "        max_length=512,\n",
        "        padding='max_length',\n",
        "        truncation=True\n",
        "    )\n",
        "\n",
        "# Tokenize the Training Data\n",
        "train_dataset = cleaned_dataset['train'].map(\n",
        "    tokenize_func,\n",
        "    batched=True\n",
        ")\n",
        "\n",
        "# Tokenize the Validation Data\n",
        "val_dataset = cleaned_dataset['validation'].map(\n",
        "    tokenize_func,\n",
        "    batched=True\n",
        ")\n",
        "\n",
        "# Tokenize the Test Data\n",
        "test_dataset = cleaned_dataset['test'].map(\n",
        "    tokenize_func,\n",
        "    batched=True\n",
        ")\n",
        "\n",
        "# Define the format for labels to ensure they match input size\n",
        "def format_labels(examples):\n",
        "    examples['labels'] = examples['label']\n",
        "    return examples\n",
        "\n",
        "train_dataset = train_dataset.map(format_labels, batched=True)\n",
        "val_dataset = val_dataset.map(format_labels, batched=True)\n",
        "test_dataset = test_dataset.map(format_labels, batched=True)\n",
        "\n",
        "\n",
        "###Step3: Model Training and Evaluation\n",
        "# Set up wandb config\n",
        "wandb.config = {\n",
        "    \"learning_rate\": 1e-8, ##!!!reduce it\n",
        "    \"epochs\": 1000,\n",
        "    \"batch_size\": 16,\n",
        "    \"model_name\": modelpath,\n",
        "    \"quantization_config\": bnb_config,\n",
        "    \"lora_config\": config\n",
        "}\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"output/distilbert_qlora\",\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    logging_steps=10,\n",
        "    save_steps=500,\n",
        "    eval_steps=187,\n",
        "    save_total_limit=3,\n",
        "    gradient_accumulation_steps=16,\n",
        "    max_steps=1875,\n",
        "    lr_scheduler_type=\"constant\",\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    learning_rate=0.0002,\n",
        "    group_by_length=True,\n",
        "    bf16=True,\n",
        "    warmup_ratio=0.03,\n",
        "    max_grad_norm=0.3,\n",
        "    report_to=\"wandb\"  # Enable logging to wandb\n",
        ")\n",
        "\n",
        "from adapters import AdapterTrainer\n",
        "from transformers import DataCollatorWithPadding\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import time\n",
        "import psutil\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer)\n",
        "\n",
        "# Custom function to log time and memory usage\n",
        "def log_time_memory():\n",
        "    # Log the current time and memory usage\n",
        "    current_time = time.time()\n",
        "    memory_info = psutil.virtual_memory()\n",
        "    wandb.log({\n",
        "        \"time\": current_time,\n",
        "        \"memory_usage\": memory_info.used / (1024 ** 2)  # Convert to MB\n",
        "    })\n",
        "\n",
        "def metrics(eval_prediction):\n",
        "    logits, labels = eval_prediction\n",
        "    # Convert logits to probabilities\n",
        "    probs = torch.nn.functional.softmax(torch.tensor(logits), dim=1).numpy()\n",
        "    auc_score = roc_auc_score(labels, probs, multi_class='ovr')\n",
        "    return {\"accuracy\": auc_score}\n",
        "\n",
        "\n",
        "trainer = AdapterTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    args=args,\n",
        "    compute_metrics=metrics\n",
        ")\n",
        "\n",
        "log_time_memory()\n",
        "trainer.train()\n",
        "trainer.save_model()\n",
        "wandb.finish()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyNB5AYbeJm+Mxxe2IMKlpbR",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0a01d4e3c33d452daa9723a04fc90c55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e081ad14de854dc884428ab1eeb05cee",
              "IPY_MODEL_9e2019b0a84e41cb9f4f1d365d90c46e"
            ],
            "layout": "IPY_MODEL_7dc2207930834e89a6338c6b632dee2f"
          }
        },
        "e081ad14de854dc884428ab1eeb05cee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d1a17cee8714460950144f8ec53f024",
            "placeholder": "​",
            "style": "IPY_MODEL_8b5f0f8019c34331b33d1b680231ae55",
            "value": "0.016 MB of 0.016 MB uploaded\r"
          }
        },
        "9e2019b0a84e41cb9f4f1d365d90c46e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ee7153aeb724fe49b58c7e761b17567",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b4f5bbc7cdc44e62a136a57e51494300",
            "value": 1
          }
        },
        "7dc2207930834e89a6338c6b632dee2f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d1a17cee8714460950144f8ec53f024": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b5f0f8019c34331b33d1b680231ae55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5ee7153aeb724fe49b58c7e761b17567": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4f5bbc7cdc44e62a136a57e51494300": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6a667649db264592a0e6313cd20516bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0d577d5664a84c0e8bdfae26f2896c64",
              "IPY_MODEL_553ad191aa7d4f75ac02165e1919d169"
            ],
            "layout": "IPY_MODEL_f514f2b3e7c941699ac363ffcec364e7"
          }
        },
        "0d577d5664a84c0e8bdfae26f2896c64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c22bc4613e549a6aa0cf49c271b9d6d",
            "placeholder": "​",
            "style": "IPY_MODEL_0cc3ea6ce2f54fa1a97f3c9f91425854",
            "value": "0.017 MB of 0.017 MB uploaded\r"
          }
        },
        "553ad191aa7d4f75ac02165e1919d169": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb7d4e2c1e1a432589e6855108a18d8e",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1a4d02d8ba7344b49b28093f3dc34be4",
            "value": 1
          }
        },
        "f514f2b3e7c941699ac363ffcec364e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c22bc4613e549a6aa0cf49c271b9d6d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0cc3ea6ce2f54fa1a97f3c9f91425854": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eb7d4e2c1e1a432589e6855108a18d8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a4d02d8ba7344b49b28093f3dc34be4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}